{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6840ca84-2e85-48c4-ba15-ac2205b8769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abcc4c08-0c86-457b-aa62-27b5cf45f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3)\n",
    "        self.norm1 = nn.InstanceNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.norm2 = nn.InstanceNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.norm3 = nn.InstanceNorm2d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = F.relu(self.norm2(self.conv2(x)))\n",
    "        x = F.relu(self.norm3(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.norm1 = nn.InstanceNorm2d(256)\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.InstanceNorm2d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.norm1(self.conv1(x)))\n",
    "        out = self.norm2(self.conv2(out))\n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.norm1 = nn.InstanceNorm2d(128)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.norm2 = nn.InstanceNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 3, kernel_size=7, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.norm1(self.deconv1(x)))\n",
    "        x = F.relu(self.norm2(self.deconv2(x)))\n",
    "        x = torch.tanh(self.conv3(x))  # Output with Tanh activation\n",
    "        return x\n",
    "\n",
    "# Enhance-Net Model\n",
    "class EnhanceNet(nn.Module):\n",
    "    def __init__(self, num_res_blocks=9):\n",
    "        super(EnhanceNet, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.residual_blocks = nn.Sequential(*[ResidualBlock() for _ in range(num_res_blocks)])\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "284a870a-c03e-4a46-8a3e-cea545d6d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: Mean Absolute Error (MAE)\n",
    "def mae_loss(output, target):\n",
    "    return torch.mean(torch.abs(output - target))\n",
    "\n",
    "# Perceptual Loss using pretrained VGG19\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features[:16].eval()\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = vgg\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        output_features = self.vgg(output)\n",
    "        target_features = self.vgg(target)\n",
    "        return mae_loss(output_features, target_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f3e717-dd77-412e-b74f-fcba50235c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.images = glob.glob(os.path.join(folder_path, '*.*')) \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")  \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c65c6-bf86-48d0-a551-9ca022be35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the model to mps\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1  \n",
    "epochs = 1\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "train_paths = '../CURE-TSR/Train/Haze-3' # Path to the training dataset\n",
    "val_paths = '../CURE-TSR/Test/Haze-3' # Path to the validation dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = CustomImageDataset(train_paths, transform=transform)\n",
    "val_dataset = CustomImageDataset(val_paths, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = EnhanceNet().to(device)\n",
    "mae_criterion = mae_loss\n",
    "perceptual_criterion = PerceptualLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Training step\n",
    "    for i, images in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_mae = mae_criterion(outputs, images)  # MAE loss\n",
    "        loss_perceptual = perceptual_criterion(outputs, images)  # Perceptual loss\n",
    "        loss = loss_mae + loss_perceptual  # Combined loss\n",
    "\n",
    "        # Backpropagation with gradient accumulation\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()  \n",
    "            optimizer.zero_grad() \n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_images in val_loader:  \n",
    "            val_images = val_images.to(device) \n",
    "            val_outputs = model(val_images)  \n",
    "\n",
    "            # Validation losses\n",
    "            val_loss_mae = mae_criterion(val_outputs, val_images)  # MAE loss\n",
    "            val_loss_perceptual = perceptual_criterion(val_outputs, val_images)  # Perceptual loss\n",
    "            val_loss += val_loss_mae.item() + val_loss_perceptual.item()  # Accumulate validation loss\n",
    "\n",
    "    # Print epoch information\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], '\n",
    "          f'Training Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Validation Loss: {val_loss / len(val_loader):.4f}')\n",
    "\n",
    "    # Update learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "# Save the trained EnhanceNet model\n",
    "torch.save(model.state_dict(), 'enhance_net_haze.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee7ed2-a6d4-4475-ab0d-65e60514e63d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
